{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary Library\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import io\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "################################# LSTM MODEL ##########################################\n",
    "#######################################################################################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "## Creating a class of nn.Module to include RNN with LSTM Units\n",
    "class LSTM_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, batch_size = 1, n_layers = 1):\n",
    "        \n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        ## Creating an embedding object to create 'embedding_size' dimensional encoding\n",
    "        self.encoder = nn.Embedding(vocab_size,embedding_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,n_layers)\n",
    "        \n",
    "        ## The output size is assigned as hidden_size * vocab_size \n",
    "        ## (Since the output is one word from the entire vocab)\n",
    "        self.linear_output = nn.Linear(hidden_size,vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        ## More variables\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "        \n",
    "    def init_weights(self):\n",
    "        random_range = 1\n",
    "        \n",
    "        ## Setting random values for different layers\n",
    "        self.encoder.weight.data.uniform_(-random_range, random_range)\n",
    "        \n",
    "        self.linear_output.bias.data.zero_()\n",
    "        self.linear_output.weight.data.uniform_(-random_range, random_range)\n",
    "\n",
    "    # Defining the forward layer    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        #Passing through the embedding layer\n",
    "        embedding_input = self.encoder(input).view(len(input), 1, -1)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embedding_input, hidden)\n",
    "        \n",
    "        decoded = self.linear_output(lstm_out.view(lstm_out.size(0)*lstm_out.size(1), lstm_out.size(2)))\n",
    "        \n",
    "        soft_max_out = F.log_softmax(decoded, dim = 1)\n",
    "        \n",
    "        #decoded.view(lstm_out.size(0), lstm_out.size(1), decoded.size(1))\n",
    "        return soft_max_out, hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters())\n",
    "        \n",
    "        return (weight.new_zeros(self.n_layers, batch_size, self.hidden_size), \\\n",
    "                    weight.new_zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "    \n",
    "######################################################################################\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Records\n",
    "train_records = []\n",
    "with io.open('trn-wiki.txt','r',encoding = 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        train_records.append(line)\n",
    "        \n",
    "        \n",
    "### Validation Records\n",
    "valid_records = []\n",
    "with io.open('dev-wiki.txt','r',encoding = 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        valid_records.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word to index\n",
    "word_to_ix = {}\n",
    "for sent in train_records:\n",
    "    for word in sent.split(\" \"):\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq.split(\" \")]\n",
    "    \n",
    "    data = idxs[:-1]\n",
    "    target = idxs[1:]\n",
    "    \n",
    "    return (torch.tensor(data, dtype=torch.long).to(device),\n",
    "            torch.tensor(target, dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    hidden = model.init_hidden(mini_batch_size)\n",
    "    \n",
    "    for batch, sent in enumerate(train_records):\n",
    "        data, targets = prepare_sequence(sent, word_to_ix)\n",
    "       \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        model.zero_grad()\n",
    "         \n",
    "        ## Calling the forward pass of the model \n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        ## Calculating the loss\n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        \n",
    "        ## Computing gradient and optimizing\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "        \n",
    "        \n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            \n",
    "            cur_loss = total_loss / log_interval\n",
    "            #elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_records) // mini_batch_size, lr,\\\n",
    "                    #elapsed * 1000 / args.log_interval, \n",
    "                        cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            #start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source,perplexity_calc = False):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    perplexity = []\n",
    "    \n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, sent in enumerate(data_source):\n",
    "            \n",
    "\n",
    "            data, targets = prepare_sequence(sent, word_to_ix)\n",
    "            output, hidden = model(data, hidden)\n",
    "     \n",
    "            output_flat = output.view(-1, vocab_size)\n",
    "            total_loss +=  criterion(output_flat, targets).item()\n",
    "            \n",
    "            hidden = repackage_hidden(hidden)\n",
    "            \n",
    "            if(perplexity_calc):\n",
    "                perplexity.append([output_flat[position,word_id].tolist() for position,word_id in enumerate(targets)])\n",
    "                \n",
    "            \n",
    "            \n",
    "    return perplexity,total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.exp(-a/(17556+1800340))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#vocab_size = len(dictionary)\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "embedding_size = 32 \n",
    "hidden_size = 32\n",
    "\n",
    "mini_batch_size = 1\n",
    "eval_batch_size = mini_batch_size\n",
    "n_layers = 1 # For single layer LSTM\n",
    "\n",
    "#### Model parameters \n",
    "\n",
    "lr = 0.1 # Learning rate\n",
    "epochs = 20 #\n",
    "gradient_clip_val = 2\n",
    "log_interval = 1000\n",
    "\n",
    "ntokens = vocab_size\n",
    "\n",
    "\n",
    "##### Creating the model object from the LSTM_Model in the models.py\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = LSTM_Model(vocab_size, embedding_size, hidden_size,n_layers = 1).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= lr) ## SGD Optimizer\n",
    "criterion = nn.CrossEntropyLoss() ## Crossentropy Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Additional Functions for training the model ##############\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/17556 batches | lr 0.10 | loss  8.42 | ppl  4547.04\n",
      "| epoch   1 |  2000/17556 batches | lr 0.10 | loss  7.67 | ppl  2144.43\n",
      "| epoch   1 |  3000/17556 batches | lr 0.10 | loss  7.49 | ppl  1782.16\n",
      "| epoch   1 |  4000/17556 batches | lr 0.10 | loss  7.58 | ppl  1953.35\n",
      "| epoch   1 |  5000/17556 batches | lr 0.10 | loss  7.42 | ppl  1661.31\n",
      "| epoch   1 |  6000/17556 batches | lr 0.10 | loss  7.31 | ppl  1501.19\n",
      "| epoch   1 |  7000/17556 batches | lr 0.10 | loss  7.37 | ppl  1587.27\n",
      "| epoch   1 |  8000/17556 batches | lr 0.10 | loss  7.32 | ppl  1514.79\n",
      "| epoch   1 |  9000/17556 batches | lr 0.10 | loss  7.27 | ppl  1435.29\n",
      "| epoch   1 | 10000/17556 batches | lr 0.10 | loss  7.10 | ppl  1217.96\n",
      "| epoch   1 | 11000/17556 batches | lr 0.10 | loss  7.17 | ppl  1303.74\n",
      "| epoch   1 | 12000/17556 batches | lr 0.10 | loss  7.07 | ppl  1180.85\n",
      "| epoch   1 | 13000/17556 batches | lr 0.10 | loss  7.21 | ppl  1358.47\n",
      "| epoch   1 | 14000/17556 batches | lr 0.10 | loss  7.13 | ppl  1252.35\n",
      "| epoch   1 | 15000/17556 batches | lr 0.10 | loss  7.03 | ppl  1129.88\n",
      "| epoch   1 | 16000/17556 batches | lr 0.10 | loss  7.16 | ppl  1286.68\n",
      "| epoch   1 | 17000/17556 batches | lr 0.10 | loss  7.04 | ppl  1145.18\n",
      "6.86947219748\n",
      "| epoch   2 |  1000/17556 batches | lr 0.10 | loss  7.02 | ppl  1117.22\n",
      "| epoch   2 |  2000/17556 batches | lr 0.10 | loss  6.85 | ppl   942.98\n",
      "| epoch   2 |  3000/17556 batches | lr 0.10 | loss  6.81 | ppl   911.00\n",
      "| epoch   2 |  4000/17556 batches | lr 0.10 | loss  6.98 | ppl  1076.17\n",
      "| epoch   2 |  5000/17556 batches | lr 0.10 | loss  6.89 | ppl   980.58\n",
      "| epoch   2 |  6000/17556 batches | lr 0.10 | loss  6.77 | ppl   870.94\n",
      "| epoch   2 |  7000/17556 batches | lr 0.10 | loss  6.91 | ppl   999.17\n",
      "| epoch   2 |  8000/17556 batches | lr 0.10 | loss  6.90 | ppl   992.07\n",
      "| epoch   2 |  9000/17556 batches | lr 0.10 | loss  6.84 | ppl   937.47\n",
      "| epoch   2 | 10000/17556 batches | lr 0.10 | loss  6.75 | ppl   856.52\n",
      "| epoch   2 | 11000/17556 batches | lr 0.10 | loss  6.82 | ppl   919.90\n",
      "| epoch   2 | 12000/17556 batches | lr 0.10 | loss  6.75 | ppl   856.82\n",
      "| epoch   2 | 13000/17556 batches | lr 0.10 | loss  6.90 | ppl   987.80\n",
      "| epoch   2 | 14000/17556 batches | lr 0.10 | loss  6.84 | ppl   931.54\n",
      "| epoch   2 | 15000/17556 batches | lr 0.10 | loss  6.78 | ppl   879.61\n",
      "| epoch   2 | 16000/17556 batches | lr 0.10 | loss  6.91 | ppl  1005.44\n",
      "| epoch   2 | 17000/17556 batches | lr 0.10 | loss  6.79 | ppl   893.02\n",
      "6.65234919171\n",
      "| epoch   3 |  1000/17556 batches | lr 0.10 | loss  6.80 | ppl   897.14\n",
      "| epoch   3 |  2000/17556 batches | lr 0.10 | loss  6.63 | ppl   755.60\n",
      "| epoch   3 |  3000/17556 batches | lr 0.10 | loss  6.59 | ppl   730.36\n",
      "| epoch   3 |  4000/17556 batches | lr 0.10 | loss  6.78 | ppl   878.43\n",
      "| epoch   3 |  5000/17556 batches | lr 0.10 | loss  6.70 | ppl   809.99\n",
      "| epoch   3 |  6000/17556 batches | lr 0.10 | loss  6.54 | ppl   695.39\n",
      "| epoch   3 |  7000/17556 batches | lr 0.10 | loss  6.71 | ppl   820.88\n"
     ]
    }
   ],
   "source": [
    "### Running the data over different epochs\n",
    "#lr = 0.1\n",
    "prev_val_loss = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    #epoch_start_time = time.time()\n",
    "    train()\n",
    "    _, val_loss = evaluate(valid_records)\n",
    "    print(val_loss)\n",
    "    \n",
    "    if(val_loss > prev_val_loss):\n",
    "        lr /= 1.5\n",
    "        \n",
    "    prev_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perp,train_score = evaluate(valid_records,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sent = len(train_perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_prob = []\n",
    "sum_words = 0\n",
    "for sent in train_perp:\n",
    "    word_prob.extend(sent)\n",
    "    sum_words += len(sent)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-np.sum(word_prob)/(sum_words+tot_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
