{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Necessary Library ######\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "################################# LSTM MODEL ##########################################\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "## Creating a class of nn.Module to include RNN with LSTM Units\n",
    "class LSTM_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, batch_size = 1, n_layers = 1):\n",
    "        \n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        ## Creating an embedding object to create 'embedding_size' dimensional encoding\n",
    "        self.encoder = nn.Embedding(vocab_size,embedding_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,n_layers)\n",
    "        \n",
    "        ## The output size is assigned as hidden_size * vocab_size \n",
    "        ## (Since the output is one word from the entire vocab)\n",
    "        self.linear_output = nn.Linear(hidden_size,vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        ## More variables\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "        \n",
    "    def init_weights(self):\n",
    "        random_range = 1\n",
    "        \n",
    "        ## Setting random values for different layers\n",
    "        self.encoder.weight.data.uniform_(-random_range, random_range)\n",
    "        \n",
    "        self.linear_output.bias.data.zero_()\n",
    "        self.linear_output.weight.data.uniform_(-random_range, random_range)\n",
    "\n",
    "    # Defining the forward layer    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        #Passing through the embedding layer\n",
    "        embedding_input = self.encoder(input).view(len(input), 1, -1)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embedding_input, hidden)\n",
    "        \n",
    "        decoded = self.linear_output(lstm_out.view(lstm_out.size(0)*lstm_out.size(1), lstm_out.size(2)))\n",
    "        \n",
    "        soft_max_out = F.log_softmax(decoded, dim = 1)\n",
    "        \n",
    "        #decoded.view(lstm_out.size(0), lstm_out.size(1), decoded.size(1))\n",
    "        return soft_max_out, hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters())\n",
    "        \n",
    "        return (weight.new_zeros(self.n_layers, batch_size, self.hidden_size), \\\n",
    "                    weight.new_zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "    \n",
    "######################################################################################\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "###### Reading the input dataset #########################\n",
    "##########################################################\n",
    "\n",
    "\n",
    "## Training Records\n",
    "train_records = []\n",
    "with io.open('trn-wiki.txt','r',encoding = 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        train_records.append(line)\n",
    "        \n",
    "        \n",
    "### Validation Records\n",
    "valid_records = []\n",
    "with io.open('dev-wiki.txt','r',encoding = 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        valid_records.append(line)\n",
    "        \n",
    "        \n",
    "### Test Records\n",
    "test_records = []\n",
    "with io.open('tst-wiki.txt','r',encoding = 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        test_records.append(line)\n",
    "        \n",
    "        \n",
    "################ Creating word to ID objects #############\n",
    "### Word to index\n",
    "word_to_ix = {}\n",
    "for sent in train_records:\n",
    "    for word in sent.split(\" \"):\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "            \n",
    "########### Preparing the sequence for training ##########\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq.split(\" \")]\n",
    "    \n",
    "    data = idxs[:-1]\n",
    "    target = idxs[1:]\n",
    "    \n",
    "    return (torch.tensor(data, dtype=torch.long).to(device),\n",
    "            torch.tensor(target, dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "################## Training Function #######################\n",
    "############################################################\n",
    "\n",
    "def train():\n",
    "    \n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    hidden = model.init_hidden(mini_batch_size)\n",
    "    \n",
    "    for batch, sent in enumerate(train_records):\n",
    "        data, targets = prepare_sequence(sent, word_to_ix)\n",
    "       \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        model.zero_grad()\n",
    "         \n",
    "        ## Calling the forward pass of the model \n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        ## Calculating the loss\n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        \n",
    "        ## Computing gradient and optimizing\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "        \n",
    "        \n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            \n",
    "            cur_loss = total_loss / log_interval\n",
    "            #elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_records) // mini_batch_size, lr,\\\n",
    "                        cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            #start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "################## Validation Function #####################\n",
    "############################################################\n",
    "\n",
    "\n",
    "def evaluate(data_source,perplexity_calc = False):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    perplexity = []\n",
    "    \n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, sent in enumerate(data_source):\n",
    "            \n",
    "\n",
    "            data, targets = prepare_sequence(sent, word_to_ix)\n",
    "            output, hidden = model(data, hidden)\n",
    "     \n",
    "            output_flat = output.view(-1, vocab_size)\n",
    "            total_loss +=  criterion(output_flat, targets).item()\n",
    "            \n",
    "            hidden = repackage_hidden(hidden)\n",
    "            \n",
    "            if(perplexity_calc):\n",
    "                perplexity.append([output_flat[position,word_id].tolist() for position,word_id in enumerate(targets)])\n",
    "            \n",
    "    return perplexity,total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "################## Input Arguments to the model ############\n",
    "############################################################\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "embedding_size = 32 \n",
    "hidden_size = 32\n",
    "\n",
    "mini_batch_size = 1\n",
    "eval_batch_size = mini_batch_size\n",
    "n_layers = 1 # For single layer LSTM\n",
    "\n",
    "#### Model parameters \n",
    "\n",
    "lr = 0.1 # Learning rate\n",
    "epochs = 20 #\n",
    "gradient_clip_val = 2\n",
    "log_interval = 1000\n",
    "\n",
    "ntokens = vocab_size\n",
    "\n",
    "\n",
    "##### Creating the model object from the LSTM_Model in the models.py\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = LSTM_Model(vocab_size, embedding_size, hidden_size,n_layers = 1).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= lr) ## SGD Optimizer\n",
    "criterion = nn.CrossEntropyLoss() ## Crossentropy Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Additional Functions for training the model #############\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/17556 batches | lr 0.10 | loss  8.40 | ppl  4462.13\n",
      "| epoch   1 |  2000/17556 batches | lr 0.10 | loss  7.68 | ppl  2156.43\n",
      "| epoch   1 |  3000/17556 batches | lr 0.10 | loss  7.48 | ppl  1780.17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7c665e9cb3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#epoch_start_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-582da806be4b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_clip_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrk_ram108/.local/lib/python2.7/site-packages/torch/nn/utils/clip_grad.pyc\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "################## Training the model ######################\n",
    "############################################################\n",
    "\n",
    "### Running the data over different epochs\n",
    "prev_val_loss = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    #epoch_start_time = time.time()\n",
    "    train()\n",
    "    _, val_loss = evaluate(valid_records)\n",
    "    print(val_loss)\n",
    "    \n",
    "    \n",
    "    ### If the validation loss \n",
    "    if(val_loss > prev_val_loss):\n",
    "        lr /= 1.5\n",
    "        \n",
    "    prev_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Getting Training, Validation and Testing results #########\n",
    "\n",
    "train_perp,train_score = evaluate(train_records,True)\n",
    "valid_perp,valid_score = evaluate(valid_records,True)\n",
    "test_perp,test_score = evaluate(test_records,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_prob = []\n",
    "sum_words = 0\n",
    "for sent in train_perp:\n",
    "    word_prob.extend(sent)\n",
    "    sum_words += len(sent)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-np.sum(word_prob)/(sum_words+tot_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Saving the model \n",
    "\n",
    "#with open('model.pt', 'wb') as f:\n",
    "#    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
