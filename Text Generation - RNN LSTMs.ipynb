{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with open(path, 'r', encoding=\"utf8\") as f:\n",
    "    ids = torch.LongTensor(tokens)\n",
    "    token = 0\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            ids[token] = dictionary.word2idx[word]\n",
    "            token += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_data = batchify(train_corpus, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dictionary)\n",
    "embedding_size = hidden_size = 32\n",
    "\n",
    "model = LSTM_Model(vocab_size, embedding_size, hidden_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 1\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(seq_size, len(source) - 1 - i)\n",
    "    \n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10\n",
    "log_interval = 10\n",
    "epoch = 1\n",
    "seq_size = 100\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    #start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_size)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        \n",
    "        #print(hidden.shape)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        #print(hidden[0].shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        \n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        \n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            #elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // seq_size, lr,\\\n",
    "                    #elapsed * 1000 / args.log_interval, \n",
    "                        cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            #start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/18003 batches | lr 10.00 | loss 10.51 | ppl 36855.89\n",
      "| epoch   1 |    20/18003 batches | lr 10.00 | loss  8.76 | ppl  6376.45\n",
      "| epoch   1 |    30/18003 batches | lr 10.00 | loss  8.06 | ppl  3166.28\n",
      "| epoch   1 |    40/18003 batches | lr 10.00 | loss  7.80 | ppl  2436.17\n",
      "| epoch   1 |    50/18003 batches | lr 10.00 | loss  7.30 | ppl  1481.30\n",
      "| epoch   1 |    60/18003 batches | lr 10.00 | loss  7.22 | ppl  1370.95\n",
      "| epoch   1 |    70/18003 batches | lr 10.00 | loss  7.47 | ppl  1750.42\n",
      "| epoch   1 |    80/18003 batches | lr 10.00 | loss  7.44 | ppl  1699.23\n",
      "| epoch   1 |    90/18003 batches | lr 10.00 | loss  7.11 | ppl  1223.72\n",
      "| epoch   1 |   100/18003 batches | lr 10.00 | loss  6.68 | ppl   792.44\n",
      "| epoch   1 |   110/18003 batches | lr 10.00 | loss  7.51 | ppl  1822.74\n",
      "| epoch   1 |   120/18003 batches | lr 10.00 | loss  7.37 | ppl  1583.55\n",
      "| epoch   1 |   130/18003 batches | lr 10.00 | loss  7.30 | ppl  1475.48\n",
      "| epoch   1 |   140/18003 batches | lr 10.00 | loss  6.85 | ppl   946.79\n",
      "| epoch   1 |   150/18003 batches | lr 10.00 | loss  7.24 | ppl  1387.93\n",
      "| epoch   1 |   160/18003 batches | lr 10.00 | loss  6.90 | ppl   991.50\n",
      "| epoch   1 |   170/18003 batches | lr 10.00 | loss  7.27 | ppl  1441.87\n",
      "| epoch   1 |   180/18003 batches | lr 10.00 | loss  6.99 | ppl  1085.85\n",
      "| epoch   1 |   190/18003 batches | lr 10.00 | loss  7.36 | ppl  1564.08\n",
      "| epoch   1 |   200/18003 batches | lr 10.00 | loss  7.28 | ppl  1454.16\n",
      "| epoch   1 |   210/18003 batches | lr 10.00 | loss  7.16 | ppl  1281.01\n",
      "| epoch   1 |   220/18003 batches | lr 10.00 | loss  7.23 | ppl  1375.71\n",
      "| epoch   1 |   230/18003 batches | lr 10.00 | loss  6.82 | ppl   914.04\n",
      "| epoch   1 |   240/18003 batches | lr 10.00 | loss  6.60 | ppl   734.10\n",
      "| epoch   1 |   250/18003 batches | lr 10.00 | loss  6.62 | ppl   749.18\n",
      "| epoch   1 |   260/18003 batches | lr 10.00 | loss  7.10 | ppl  1206.31\n",
      "| epoch   1 |   270/18003 batches | lr 10.00 | loss  7.41 | ppl  1647.67\n",
      "| epoch   1 |   280/18003 batches | lr 10.00 | loss  7.00 | ppl  1093.81\n",
      "| epoch   1 |   290/18003 batches | lr 10.00 | loss  6.70 | ppl   814.94\n",
      "| epoch   1 |   300/18003 batches | lr 10.00 | loss  6.64 | ppl   763.14\n",
      "| epoch   1 |   310/18003 batches | lr 10.00 | loss  6.84 | ppl   937.83\n",
      "| epoch   1 |   320/18003 batches | lr 10.00 | loss  6.86 | ppl   955.72\n",
      "| epoch   1 |   330/18003 batches | lr 10.00 | loss  6.50 | ppl   665.72\n",
      "| epoch   1 |   340/18003 batches | lr 10.00 | loss  6.52 | ppl   678.58\n",
      "| epoch   1 |   350/18003 batches | lr 10.00 | loss  7.03 | ppl  1127.76\n",
      "| epoch   1 |   360/18003 batches | lr 10.00 | loss  7.36 | ppl  1566.83\n",
      "| epoch   1 |   370/18003 batches | lr 10.00 | loss  7.03 | ppl  1126.95\n",
      "| epoch   1 |   380/18003 batches | lr 10.00 | loss  7.20 | ppl  1339.34\n",
      "| epoch   1 |   390/18003 batches | lr 10.00 | loss  6.92 | ppl  1010.05\n",
      "| epoch   1 |   400/18003 batches | lr 10.00 | loss  7.13 | ppl  1246.93\n",
      "| epoch   1 |   410/18003 batches | lr 10.00 | loss  6.71 | ppl   816.89\n",
      "| epoch   1 |   420/18003 batches | lr 10.00 | loss  6.77 | ppl   869.01\n",
      "| epoch   1 |   430/18003 batches | lr 10.00 | loss  6.74 | ppl   848.73\n",
      "| epoch   1 |   440/18003 batches | lr 10.00 | loss  6.71 | ppl   820.94\n",
      "| epoch   1 |   450/18003 batches | lr 10.00 | loss  6.79 | ppl   889.06\n",
      "| epoch   1 |   460/18003 batches | lr 10.00 | loss  6.92 | ppl  1014.10\n",
      "| epoch   1 |   470/18003 batches | lr 10.00 | loss  6.84 | ppl   934.58\n",
      "| epoch   1 |   480/18003 batches | lr 10.00 | loss  6.70 | ppl   811.58\n",
      "| epoch   1 |   490/18003 batches | lr 10.00 | loss  6.29 | ppl   541.82\n",
      "| epoch   1 |   500/18003 batches | lr 10.00 | loss  6.19 | ppl   490.02\n",
      "| epoch   1 |   510/18003 batches | lr 10.00 | loss  6.80 | ppl   898.48\n",
      "| epoch   1 |   520/18003 batches | lr 10.00 | loss  6.63 | ppl   758.05\n",
      "| epoch   1 |   530/18003 batches | lr 10.00 | loss  6.88 | ppl   973.89\n",
      "| epoch   1 |   540/18003 batches | lr 10.00 | loss  6.48 | ppl   655.20\n",
      "| epoch   1 |   550/18003 batches | lr 10.00 | loss  6.83 | ppl   920.76\n",
      "| epoch   1 |   560/18003 batches | lr 10.00 | loss  6.67 | ppl   785.05\n",
      "| epoch   1 |   570/18003 batches | lr 10.00 | loss  6.71 | ppl   820.50\n",
      "| epoch   1 |   580/18003 batches | lr 10.00 | loss  6.53 | ppl   688.63\n",
      "| epoch   1 |   590/18003 batches | lr 10.00 | loss  6.48 | ppl   648.97\n",
      "| epoch   1 |   600/18003 batches | lr 10.00 | loss  6.68 | ppl   794.58\n",
      "| epoch   1 |   610/18003 batches | lr 10.00 | loss  6.91 | ppl  1001.84\n",
      "| epoch   1 |   620/18003 batches | lr 10.00 | loss  6.62 | ppl   753.12\n",
      "| epoch   1 |   630/18003 batches | lr 10.00 | loss  7.00 | ppl  1098.54\n",
      "| epoch   1 |   640/18003 batches | lr 10.00 | loss  6.81 | ppl   904.25\n",
      "| epoch   1 |   650/18003 batches | lr 10.00 | loss  6.41 | ppl   609.16\n",
      "| epoch   1 |   660/18003 batches | lr 10.00 | loss  6.35 | ppl   574.83\n",
      "| epoch   1 |   670/18003 batches | lr 10.00 | loss  6.50 | ppl   662.38\n",
      "| epoch   1 |   680/18003 batches | lr 10.00 | loss  6.48 | ppl   652.48\n",
      "| epoch   1 |   690/18003 batches | lr 10.00 | loss  6.72 | ppl   828.61\n",
      "| epoch   1 |   700/18003 batches | lr 10.00 | loss  6.67 | ppl   786.42\n",
      "| epoch   1 |   710/18003 batches | lr 10.00 | loss  6.46 | ppl   639.41\n",
      "| epoch   1 |   720/18003 batches | lr 10.00 | loss  6.25 | ppl   515.73\n",
      "| epoch   1 |   730/18003 batches | lr 10.00 | loss  5.89 | ppl   360.38\n",
      "| epoch   1 |   740/18003 batches | lr 10.00 | loss  6.40 | ppl   600.28\n",
      "| epoch   1 |   750/18003 batches | lr 10.00 | loss  6.99 | ppl  1085.24\n",
      "| epoch   1 |   760/18003 batches | lr 10.00 | loss  6.51 | ppl   669.74\n",
      "| epoch   1 |   770/18003 batches | lr 10.00 | loss  6.42 | ppl   611.66\n",
      "| epoch   1 |   780/18003 batches | lr 10.00 | loss  6.19 | ppl   485.64\n",
      "| epoch   1 |   790/18003 batches | lr 10.00 | loss  6.60 | ppl   736.31\n",
      "| epoch   1 |   800/18003 batches | lr 10.00 | loss  6.50 | ppl   663.13\n",
      "| epoch   1 |   810/18003 batches | lr 10.00 | loss  6.36 | ppl   578.84\n",
      "| epoch   1 |   820/18003 batches | lr 10.00 | loss  7.16 | ppl  1292.04\n",
      "| epoch   1 |   830/18003 batches | lr 10.00 | loss  6.70 | ppl   810.18\n",
      "| epoch   1 |   840/18003 batches | lr 10.00 | loss  6.68 | ppl   793.32\n",
      "| epoch   1 |   850/18003 batches | lr 10.00 | loss  6.62 | ppl   750.26\n",
      "| epoch   1 |   860/18003 batches | lr 10.00 | loss  6.51 | ppl   672.36\n",
      "| epoch   1 |   870/18003 batches | lr 10.00 | loss  5.76 | ppl   318.01\n",
      "| epoch   1 |   880/18003 batches | lr 10.00 | loss  6.05 | ppl   422.36\n",
      "| epoch   1 |   890/18003 batches | lr 10.00 | loss  5.61 | ppl   271.81\n",
      "| epoch   1 |   900/18003 batches | lr 10.00 | loss  5.67 | ppl   290.64\n",
      "| epoch   1 |   910/18003 batches | lr 10.00 | loss  6.08 | ppl   435.02\n",
      "| epoch   1 |   920/18003 batches | lr 10.00 | loss  6.05 | ppl   424.63\n",
      "| epoch   1 |   930/18003 batches | lr 10.00 | loss  6.21 | ppl   497.64\n",
      "| epoch   1 |   940/18003 batches | lr 10.00 | loss  6.45 | ppl   634.07\n",
      "| epoch   1 |   950/18003 batches | lr 10.00 | loss  6.88 | ppl   970.60\n",
      "| epoch   1 |   960/18003 batches | lr 10.00 | loss  6.43 | ppl   621.38\n",
      "| epoch   1 |   970/18003 batches | lr 10.00 | loss  6.72 | ppl   832.49\n",
      "| epoch   1 |   980/18003 batches | lr 10.00 | loss  6.72 | ppl   827.98\n",
      "| epoch   1 |   990/18003 batches | lr 10.00 | loss  6.23 | ppl   508.20\n",
      "| epoch   1 |  1000/18003 batches | lr 10.00 | loss  6.65 | ppl   772.37\n",
      "| epoch   1 |  1010/18003 batches | lr 10.00 | loss  6.57 | ppl   716.87\n",
      "| epoch   1 |  1020/18003 batches | lr 10.00 | loss  6.50 | ppl   661.96\n",
      "| epoch   1 |  1030/18003 batches | lr 10.00 | loss  5.99 | ppl   398.60\n",
      "| epoch   1 |  1040/18003 batches | lr 10.00 | loss  6.47 | ppl   643.75\n",
      "| epoch   1 |  1050/18003 batches | lr 10.00 | loss  6.23 | ppl   506.21\n",
      "| epoch   1 |  1060/18003 batches | lr 10.00 | loss  6.04 | ppl   418.30\n",
      "| epoch   1 |  1070/18003 batches | lr 10.00 | loss  6.87 | ppl   966.85\n",
      "| epoch   1 |  1080/18003 batches | lr 10.00 | loss  6.81 | ppl   906.81\n",
      "| epoch   1 |  1090/18003 batches | lr 10.00 | loss  5.87 | ppl   353.79\n",
      "| epoch   1 |  1100/18003 batches | lr 10.00 | loss  6.27 | ppl   525.99\n",
      "| epoch   1 |  1110/18003 batches | lr 10.00 | loss  6.00 | ppl   402.00\n",
      "| epoch   1 |  1120/18003 batches | lr 10.00 | loss  6.21 | ppl   498.95\n",
      "| epoch   1 |  1130/18003 batches | lr 10.00 | loss  6.15 | ppl   468.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1140/18003 batches | lr 10.00 | loss  6.42 | ppl   614.47\n",
      "| epoch   1 |  1150/18003 batches | lr 10.00 | loss  5.93 | ppl   375.14\n",
      "| epoch   1 |  1160/18003 batches | lr 10.00 | loss  6.34 | ppl   566.62\n",
      "| epoch   1 |  1170/18003 batches | lr 10.00 | loss  6.72 | ppl   826.42\n",
      "| epoch   1 |  1180/18003 batches | lr 10.00 | loss  6.53 | ppl   683.58\n",
      "| epoch   1 |  1190/18003 batches | lr 10.00 | loss  6.72 | ppl   825.06\n",
      "| epoch   1 |  1200/18003 batches | lr 10.00 | loss  6.33 | ppl   562.66\n",
      "| epoch   1 |  1210/18003 batches | lr 10.00 | loss  6.31 | ppl   552.69\n",
      "| epoch   1 |  1220/18003 batches | lr 10.00 | loss  6.02 | ppl   412.07\n",
      "| epoch   1 |  1230/18003 batches | lr 10.00 | loss  6.21 | ppl   499.77\n",
      "| epoch   1 |  1240/18003 batches | lr 10.00 | loss  6.39 | ppl   594.41\n",
      "| epoch   1 |  1250/18003 batches | lr 10.00 | loss  7.00 | ppl  1096.93\n",
      "| epoch   1 |  1260/18003 batches | lr 10.00 | loss  6.64 | ppl   763.02\n",
      "| epoch   1 |  1270/18003 batches | lr 10.00 | loss  6.55 | ppl   701.87\n",
      "| epoch   1 |  1280/18003 batches | lr 10.00 | loss  6.22 | ppl   502.83\n",
      "| epoch   1 |  1290/18003 batches | lr 10.00 | loss  6.25 | ppl   515.84\n",
      "| epoch   1 |  1300/18003 batches | lr 10.00 | loss  6.39 | ppl   594.84\n",
      "| epoch   1 |  1310/18003 batches | lr 10.00 | loss  5.81 | ppl   332.78\n",
      "| epoch   1 |  1320/18003 batches | lr 10.00 | loss  6.33 | ppl   561.88\n",
      "| epoch   1 |  1330/18003 batches | lr 10.00 | loss  6.52 | ppl   681.05\n",
      "| epoch   1 |  1340/18003 batches | lr 10.00 | loss  6.91 | ppl   997.42\n",
      "| epoch   1 |  1350/18003 batches | lr 10.00 | loss  6.33 | ppl   562.64\n",
      "| epoch   1 |  1360/18003 batches | lr 10.00 | loss  6.69 | ppl   803.20\n",
      "| epoch   1 |  1370/18003 batches | lr 10.00 | loss  6.83 | ppl   927.06\n",
      "| epoch   1 |  1380/18003 batches | lr 10.00 | loss  6.59 | ppl   727.90\n",
      "| epoch   1 |  1390/18003 batches | lr 10.00 | loss  6.67 | ppl   790.36\n",
      "| epoch   1 |  1400/18003 batches | lr 10.00 | loss  6.55 | ppl   702.32\n",
      "| epoch   1 |  1410/18003 batches | lr 10.00 | loss  6.44 | ppl   626.11\n",
      "| epoch   1 |  1420/18003 batches | lr 10.00 | loss  6.01 | ppl   405.53\n",
      "| epoch   1 |  1430/18003 batches | lr 10.00 | loss  5.68 | ppl   294.39\n",
      "| epoch   1 |  1440/18003 batches | lr 10.00 | loss  6.57 | ppl   711.22\n",
      "| epoch   1 |  1450/18003 batches | lr 10.00 | loss  6.47 | ppl   648.22\n",
      "| epoch   1 |  1460/18003 batches | lr 10.00 | loss  5.85 | ppl   346.89\n",
      "| epoch   1 |  1470/18003 batches | lr 10.00 | loss  5.48 | ppl   238.95\n",
      "| epoch   1 |  1480/18003 batches | lr 10.00 | loss  6.74 | ppl   842.10\n",
      "| epoch   1 |  1490/18003 batches | lr 10.00 | loss  5.98 | ppl   396.25\n",
      "| epoch   1 |  1500/18003 batches | lr 10.00 | loss  6.26 | ppl   524.08\n",
      "| epoch   1 |  1510/18003 batches | lr 10.00 | loss  6.21 | ppl   495.49\n",
      "| epoch   1 |  1520/18003 batches | lr 10.00 | loss  6.69 | ppl   801.22\n",
      "| epoch   1 |  1530/18003 batches | lr 10.00 | loss  6.63 | ppl   756.60\n",
      "| epoch   1 |  1540/18003 batches | lr 10.00 | loss  6.10 | ppl   447.11\n",
      "| epoch   1 |  1550/18003 batches | lr 10.00 | loss  6.24 | ppl   515.22\n",
      "| epoch   1 |  1560/18003 batches | lr 10.00 | loss  6.27 | ppl   527.27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-314-725a201646b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-313-6ec659efe6a9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 862\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1550\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[0;32m    973\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
